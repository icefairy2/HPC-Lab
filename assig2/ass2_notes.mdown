# Assignment 2
## Part 1:
### Shared memory pi-calculation
#### 1. Serial implementation
Code that integrates function 1/(1+x^2) over [0, 1].
```c++
//function to be integrated
double phi(double x) {
    return 1 / (1 + x*x);
}

int main(int argc, char** argv)
{
    int i;
    double h, y, sum;
    //number of partitions
    long n = 100000000;

    h = 1. / n;

    sum = 0;

    for (i = 0; i <= n; i++)
    {
        //calculate function value at current partition
        y = phi(i*h);
        //add current function value to sum
        sum += y;
    }
    sum *= 4. * h;
	//result of integration is in sum

    return 0;
}
```

#### 2. Parralelized application
With the critical directive, the for loop becomes:
```c++
#pragma omp parallel for private(y), shared(sum)
    for (i = 0; i <= n; i++)
    {
        //calculate function value at current partition
        y = phi(i*h);
#pragma omp critical
        //add current function value to sum
        sum += y;
    }
```
Using the reduction clause, the for loop looks like the following:
```c++
#pragma omp parallel for private(y), reduction(+: sum)
    for (i = 0; i <= n; i++)
    {
        //calculate function value at current partition
        y = phi(i*h);
        //add current function value to sum
        sum += y;
    }
```

#### 3. Scaling study
The time values are in microseconds.

##### Weak scaling using the critical directive
n = 25 000 000.

N/Nr of threads: n/16.
Time of execution (s): 17.931.

N/Nr of threads: 2n/32.
Time of execution (s): 36.884.

N/Nr of threads: 4n/64.
Time of execution (s): 76.407.

N/Nr of threads: 8n/128.
Time of execution (s): 149.951.

##### Strong scaling using the critical directive
n = 100 000 000.

Nr of threads: 16.
Time of execution (s): 73.772.

Nr of threads: 32.
Time of execution (s): 73.902.

Nr of threads: 64.
Time of execution (s): 77.451.

Nr of threads: 128.
Time of execution (s): 78.451.

Because every single thread has to access the critical zone sequentially, the time of execution slows down by using the critical directive.


##### Weak scaling using the reduction clause
n = 250 000 000.

N/Nr of threads: n/16.
Time of execution (s): 0.176.

N/Nr of threads: 2n/32.
Time of execution (s): 0.181.

N/Nr of threads: 4n/64.
Time of execution (s): 0.202.

N/Nr of threads: 8n/128.
Time of execution (s): 0.302.

##### Strong scaling using the reduction clause
n = 1 000 000 000.

Nr of threads: 16.
Time of execution (s): 0.610.

Nr of threads: 32.
Time of execution (s): 0.325.

Nr of threads: 64.
Time of execution (s): 0.196.

Nr of threads: 128.
Time of execution (s): 0.188.

###Conclusion
Using the critical directive kills performance because threads have to access the critical section one after another.
Parallelizing with the reduction clause leads to faster execution once more threads are introduced.

Strong scaling shows that if the problem is big enough adding more threads to parallelize it works up to a certain point. It will have diminishing returns once the problem chunks are so small that the overhead of adding more threads doesnâ€™t help anymore.

## Part 2

### STREAM benchmark
#### 1. The STREAM benchmark allocates 3 arrays and performs 3 sub-benchmarks on them. These benchmarks consist in O(n) loops over the data which perform different operations, as follows:

Copy
```c
#pragma omp parallel for
for (j=0; j<STREAM_ARRAY_SIZE; j++)
    c[j] = a[j];
```

Scale
```c
#pragma omp parallel for
for (j=0; j<STREAM_ARRAY_SIZE; j++)
    b[j] = scalar*c[j];
```

Add
```c
#pragma omp parallel for
for (j=0; j<STREAM_ARRAY_SIZE; j++)
    c[j] = a[j]+b[j];
```

Triad
```c
#pragma omp parallel for
for (j=0; j<STREAM_ARRAY_SIZE; j++)
    a[j] = b[j]+scalar*c[j];
```

#### 2. We can pass the array size as a compile time definition -DSTREAM_ARRAY_SIZE=80000000. This will use ~610 MB per array (1.8 GB in total).

#### 3. Flat vs Cache, Quadrant vs snc4
In **cache mode**, MCDRAM acts as L3 direct mapped cache. Using it is transparent to the user. This mode is suitable for legacy applications which are hard to modify in order to exploit flat mode. Applications which have good spatial and temporal locality can achieve peak performance.
In **flat mode**, MCDRAM and DDR4 can be allocated selectively using Memkind library. This gives the user better control over the data that goes into the high bandwidth memory. It is highly useful for applications which are limited by DDR4 bandwidth.

In **quadrant** mode, the chip is divided into 4 virtual quadrants. In this mode the tag directory and memory channel are located in the same quadrant. This mode is transparent to the user and thus it is easier to use.
In **SNC4** mode, each quadrant is exposed as a separate NUMA node. Software must be optimized for the NUMA architecture, but an optimized application will render best performance.

#### 4 Running the benchmark on the cluster with 64 threads yields the following:

##### quad,cache
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:    |      228009.4  |   0.005730  |   0.005614 |    0.005889
Scale:   |      233666.0  |   0.005561  |   0.005478  |   0.005701
Add:     |      146352.8  |   0.013542  |   0.013119  |   0.013960
Triad:   |      266622.4  |   0.007758   |  0.007201   |  0.008317

##### quad, flat
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:     |      76583.2  |   0.017029   |  0.016714   | 0.022321
Scale:    |      76600.6  |   0.017015  |   0.016710   |  0.021970
Add:      |      82431.5  |   0.024193  |   0.023292  |   0.028823
Triad:     |     82230.3  |   0.023611  |   0.023349  |   0.027358

##### snc4, cache
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:      |    218988.0  |   0.005921   |  0.005845  |   0.005991
Scale:     |    217356.6  |   0.005956   |  0.005889  |   0.006097
Add:      |     185631.5  |   0.011186  |   0.010343  |   0.011798
Triad:    |     271760.0  |   0.007150  |   0.007065  |   0.007254

##### snc4, flat
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:      |     12225.0  |   0.105086  |  0.104704 |    0.106260
Scale:    |      12084.6   |  0.106379  |   0.105920 |    0.107742
Add:     |       13078.7   |  0.147689  |   0.146804  |   0.152449
Triad:    |      13013.2 |    0.148147   |  0.147543 |    0.148948

Using flat mode, bandwidth is lower when compared to cache mode, as memory is allocated by default in DDR4. The application has to explicitly allocate memory in MCDRAM, whereas in cache mode, MCDRAM is used as an L3 cache.

#### 5. Flat vs cache on stream

Flat bandwidth is worse than cache as the memory is allocated by default in DDR4. By using `numactl -m 1` to allocate the data in MCDRAM, the application has peak performance bandwidth.

##### quad, flat
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:    |      221682.6  |   0.005875   |  0.005774  |  0.005953
Scale:    |     307697.7   |  0.004223  |   0.004160  |   0.004293
Add:      |     220740.7   |  0.008891   |  0.008698   |  0.009048
Triad:     |    370306.9  |   0.005228   |  0.005185  |   0.005280

##### snc4, flat
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:     |     223110.5  |   0.005863  |   0.005737  |   0.005953
Scale:    |     310815.1  |   0.004213  |   0.004118  |   0.004266
Add:      |     222196.4   |  0.008890 |   0.008641  |   0.009032
Triad:    |     369728.8  |   0.005229   |  0.005193  |   0.005274


## Part 4
### Part 1: Suitable values for _mr_, _nr_, _mc_ and kc
For reference the algorithm is described here in short:
C = A x B where sizeOf(A)=sizeOf(B) = S x S
1. Divide A into a block of size mc x kc and B into a panel of kc x S
2. Multiply _mr_ rows of the block A with _nr_ columns of the panel B.
3. This gives a _mr_ x _nr_ block of B.

According to the paper referred to in the exercise the idea is that _mr_ x _nr_ remain in registers to amortize costs of not having packed matrix C. KNL has a total of 32 x 512bit registers per hardware thread. Each register can pack 8 doubles. Hence if we use half the registers to allow space for prefetching A and B we have a total of 16x8 doubles that can be stored. Typically _mr_ equal to _nr_ is preferred. Hence we chose _mr_=_nr_=8 which gave us the best performance.

Elements of B are reused and hence must remain in L1 cache. As a result nr x kc must be large enough to fill up atleast half of the cache. nr is already small from what we chose above so kc needs to be sufficiently large. The L1 data cache for KNL is 32KB per core. In order for elements of B to not be evicted from L1 it needs to occupy less that 50% of L1. Keeping this consideration in mind we chose kc to 256. However after results kc=128 gave better results specially after parallelization. One possible reason for this is that each core is shared by four hardware threads which means L1 cache available to a single hardware thread is further reduced. Finally mc is chosen in such a way that it fill up most of the L2 cache. For us the experience was slightly different and a smaller mc of 128 functioned better than the recommended 256-512.

So to summarise we the following variables were chose: **mr=8nr=8mc=128kc=128**. With this configuration our **GEMM had a peak performance of about 20 GFLOPS**. While the microkernel alone had a peak performance of about 33 GFLOPS (libxsmm).

### Part 2: Parallelization
Setting the number of threads (`OMP_NUM_THREADS`) to 64 gave the best results. Further increase in threads decreased the performance. Also, **4 threads per team** seem gave the best performance. This is because of hyperthreading and better L1 cache utilization for the packed B matrix (Since L1 is shared between hyperthreads).
This settings gave us a **peak performance of about 334 GFLOPS**

### Part 3: Strong scaling analyses <TODO>
