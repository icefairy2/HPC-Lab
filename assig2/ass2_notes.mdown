# Assignment 2
## Part 1:
### Shared memory pi-calculation
#### 1. Serial implementation
Code that integrates function 1/(1+x^2) over [0, 1].
```c++
//function to be integrated
double phi(double x) {
    return 1 / (1 + x*x);
}

int main(int argc, char** argv)
{
    int i;
    double h, y, sum;
    //number of partitions
    long n = 100000000;

    h = 1. / n;

    sum = 0;

    for (i = 0; i <= n; i++)
    {
        //calculate function value at current partition
        y = phi(i*h);
        //add current function value to sum
        sum += y;
    }
    sum *= 4. * h;
	//result of integration is in sum

    return 0;
}
```

#### 2. Parralelized application
With the critical directive, the for loop becomes:
```c++
#pragma omp parallel for private(y), shared(sum)
    for (i = 0; i <= n; i++)
    {
        //calculate function value at current partition
        y = phi(i*h);
#pragma omp critical
        //add current function value to sum
        sum += y;
    }
```
Using the reduction clause, the for loop looks like the following:
```c++
#pragma omp parallel for private(y), reduction(+: sum)
    for (i = 0; i <= n; i++)
    {
        //calculate function value at current partition
        y = phi(i*h);
        //add current function value to sum
        sum += y;
    }
```

#### 3. Scaling study
The time values are in microseconds.

##### Weak scaling using the critical directive
n = 25 000 000.

N/Nr of threads: n/16.
Time of execution (s): 17.931.

N/Nr of threads: 2n/32.
Time of execution (s): 36.884.

N/Nr of threads: 4n/64.
Time of execution (s): 76.407.

N/Nr of threads: 8n/128.
Time of execution (s): 149.951.

##### Strong scaling using the critical directive
n = 100 000 000.

Nr of threads: 16.
Time of execution (s): 73.772.

Nr of threads: 32.
Time of execution (s): 73.902.

Nr of threads: 64.
Time of execution (s): 77.451.

Nr of threads: 128.
Time of execution (s): 78.451.

Because every single thread has to access the critical zone sequentially, the time of execution slows down by using the critical directive.


##### Weak scaling using the reduction clause
n = 250 000 000.

N/Nr of threads: n/16.
Time of execution (s): 0.176.

N/Nr of threads: 2n/32.
Time of execution (s): 0.181.

N/Nr of threads: 4n/64.
Time of execution (s): 0.202.

N/Nr of threads: 8n/128.
Time of execution (s): 0.302.

##### Strong scaling using the reduction clause
n = 1 000 000 000.

Nr of threads: 16.
Time of execution (s): 0.610.

Nr of threads: 32.
Time of execution (s): 0.325.

Nr of threads: 64.
Time of execution (s): 0.196.

Nr of threads: 128.
Time of execution (s): 0.188.

###Conclusion
Using the critical directive kills performance because threads have to access the critical section one after another.
Parallelizing with the reduction clause leads to faster execution once more threads are introduced.

Strong scaling shows that if the problem is big enough adding more threads to parallelize it works up to a certain point. It will have diminishing returns once the problem chunks are so small that the overhead of adding more threads doesnâ€™t help anymore.

## Part 2

### STREAM benchmark
#### 1. The STREAM benchmark allocates 3 arrays and performs 3 sub-benchmarks on them. These benchmarks consist in O(n) loops over the data which perform different operations, as follows:

Copy
```c
#pragma omp parallel for
for (j=0; j<STREAM_ARRAY_SIZE; j++)
    c[j] = a[j];
```

Scale
```c
#pragma omp parallel for
for (j=0; j<STREAM_ARRAY_SIZE; j++)
    b[j] = scalar*c[j];
```

Add
```c
#pragma omp parallel for
for (j=0; j<STREAM_ARRAY_SIZE; j++)
    c[j] = a[j]+b[j];
```

Triad
```c
#pragma omp parallel for
for (j=0; j<STREAM_ARRAY_SIZE; j++)
    a[j] = b[j]+scalar*c[j];
```

#### 2. We can pass the array size as a compile time definition -DSTREAM_ARRAY_SIZE=80000000. This will use ~610 MB per array (1.8 GB in total).

#### 3. Flat vs Cache, Quadrant vs snc4
In **cache mode**, MCDRAM acts as L3 direct mapped cache. Using it is transparent to the user. This mode is suitable for legacy applications which are hard to modify in order to exploit flat mode. Applications which have good spatial and temporal locality can achieve peak performance.
In **flat mode**, MCDRAM and DDR4 can be allocated selectively using Memkind library. This gives the user better control over the data that goes into the high bandwidth memory. It is highly useful for applications which are limited by DDR4 bandwidth.

In **quadrant** mode, the chip is divided into 4 virtual quadrants. In this mode the tag directory and memory channel are located in the same quadrant. This mode is transparent to the user and thus it is easier to use.
In **SNC4** mode, each quadrant is exposed as a separate NUMA node. Software must be optimized for the NUMA architecture, but an optimized application will render best performance.

#### 4 Running the benchmark on the cluster with 64 threads yields the following:

##### quad,cache
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:    |      228009.4  |   0.005730  |   0.005614 |    0.005889
Scale:   |      233666.0  |   0.005561  |   0.005478  |   0.005701
Add:     |      146352.8  |   0.013542  |   0.013119  |   0.013960
Triad:   |      266622.4  |   0.007758   |  0.007201   |  0.008317

##### quad, flat
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:     |      76583.2  |   0.017029   |  0.016714   | 0.022321
Scale:    |      76600.6  |   0.017015  |   0.016710   |  0.021970
Add:      |      82431.5  |   0.024193  |   0.023292  |   0.028823
Triad:     |     82230.3  |   0.023611  |   0.023349  |   0.027358

##### snc4, cache
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:      |    218988.0  |   0.005921   |  0.005845  |   0.005991
Scale:     |    217356.6  |   0.005956   |  0.005889  |   0.006097
Add:      |     185631.5  |   0.011186  |   0.010343  |   0.011798
Triad:    |     271760.0  |   0.007150  |   0.007065  |   0.007254

##### snc4, flat
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:      |     12225.0  |   0.105086  |  0.104704 |    0.106260
Scale:    |      12084.6   |  0.106379  |   0.105920 |    0.107742
Add:     |       13078.7   |  0.147689  |   0.146804  |   0.152449
Triad:    |      13013.2 |    0.148147   |  0.147543 |    0.148948

Using flat mode, bandwidth is lower when compared to cache mode, as memory is allocated by default in DDR4. The application has to explicitly allocate memory in MCDRAM, whereas in cache mode, MCDRAM is used as an L3 cache.

#### 5. Flat vs cache on stream

Flat bandwidth is worse than cache as the memory is allocated by default in DDR4. By using `numactl -m 1` to allocate the data in MCDRAM, the application has peak performance bandwidth.

##### quad, flat
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:    |      221682.6  |   0.005875   |  0.005774  |  0.005953
Scale:    |     307697.7   |  0.004223  |   0.004160  |   0.004293
Add:      |     220740.7   |  0.008891   |  0.008698   |  0.009048
Triad:     |    370306.9  |   0.005228   |  0.005185  |   0.005280

##### snc4, flat
Function  |  Best Rate MB/s | Avg time  |   Min time  |   Max time
--- | --- | --- | --- | ---
Copy:     |     223110.5  |   0.005863  |   0.005737  |   0.005953
Scale:    |     310815.1  |   0.004213  |   0.004118  |   0.004266
Add:      |     222196.4   |  0.008890 |   0.008641  |   0.009032
Triad:    |     369728.8  |   0.005229   |  0.005193  |   0.005274
