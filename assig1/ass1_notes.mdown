# Assignment 1
## Part 1:
### What is a module system and how do you use it?
#### Definition
It is a utility for managing application-specific environment settings. It helps manage the user environment variables such as PATH, MANPATH, LD_LIBRARY_PATH etc.
#### Usage:
To load a module _**module load** \<package_name\>_.  
To unload module _**module unload** \<package_name\>_.

For example if you want to load gcc 6:  
_**module load** gcc/6_  
if now you wish to switch to another version of gcc:  
_**module switch** gcc gcc/5_  
Or if you want to disable gcc module entirely:  
_**module unload** gcc_

### How can you execute programs on the clusters's compute nodes? Describe the interactive mode and the batch mode.
Jobs are submitted to SLURM (Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters).
Jobs can be submitted in two modes:
1. Using an inetractive SLURM shell
For program testing and short runs the following sequnce of commands can be used: First salloc is invoked to reserve the needed resources. Then, mpiexec can be used to start up a program on these resources.

2. Using a SLURM batch script
Write a script as given according to SLURM documentation and submit with sbatch

### Write a simple "hello world" program
The program was compiled as ex1, and the script used to run in on mpp3 is hw-script.sh. The output is located in _myjob.7033.mpp3r01c02s02.out_

## Part 2
### Why is it important to align data structures?
Because it helps with efficiency by reducing the number of instructions required to load an element into a register from a memory address. Also vectorization is possible only if addresses are aligned.

### Which kind of obstacles prevent vectorization?
1. Non contiguous memory access such as loops with non unit stride or indirect addressing.
2. Data dependencies.

### Is there a way to assist the compiler through language extensions? If yes, please give details.
1. Pragmas: 
    a. #pragma ivdep #pragma novector
    b. #pragma vector always, #pragma loop count (n)
    c. #pragma vectory align (asserts data is aligned for intel SSE instructions)
    d. #pragma vector non temporal (data doesn't need to be cached)
2. Keywords
    a. restrict: memory referenced by a pointer is not aliased
3. Options/Switches
    a. Interprocedural optimization (IPO) - may help with inlining. Using Q[ip] or Q[ipo].
    b. Disambiguation of pointers and arrays: Using options -fno-alias asserts there is no aliasing of memory references.
4. High level optimization - Additional optimizations that help with vectorization. Enable it with switch o3

### Which loop optimisations are performed by the compiler in order to vectorise and pipeline loops?
Other than optimizations such as unrolling, loop collapsing or interchange, fusion (join two loops), fission (break two loops) which are useful for parallelization the compiler also uses the following techniques to allow auto vectorization.
1. Inlining function calls: In general loops are not vectorizable if they have function calls. However if this function itself is vectorizable then it may get inlined and allow the whole function to be vectorized.
2. Conditional assignments within a loop can be vectorized using masks. For example:

```C++
void quad(int length, float *a, float *b, 
            float *c, float *restrict x1, float *restrict x2) {
    for (int i=0; i<length; i++) {
        float s = b[i]*b[i] - 4*a[i]*c[i];
        if ( s >= 0 ) {
            s = sqrt(s) ;
            x2[i] = (-b[i]+s)/(2.*a[i]);
            x1[i] = (-b[i]-s)/(2.*a[i]);
        } else {
            x2[i] = 0.;
            x1[i] = 0.;
        }
    }
}
```
For this example all branches are evaluated but finally based on masks only the required ones are stored.
3. Strip mining and blocking: Uses loop transformation for SIMD encodings of loops (for example by using intrinsics). Transforms in two ways:
    * By increasing locality (temporal and spatial) of the data cache if its reusable.
    * By reducing the number of iterations of the loop by a factor of the length of each vector, or number of operations being performed per SIMD operation.
The second point is illustrated through the following example:

#### Before vectorization:
```C++
i=0;
while(i<n) {
    // Original loop code
    a[i]=b[i]+c[i];
    ++i;
}
```

#### After vectorization:
```C++
// The vectorizer generates the following two loops
i=0;
while(i<(n-n%4)) {
    // Vector strip-mined loop
    // Subscript [i:i+3] denotes SIMD execution
    a[i:i+3]=b[i:i+3]+c[i:i+3];
    i=i+4;
}
// Cleanup loop
while(i<n) {
    // Scalar clean-up loop
    a[i]=b[i]+c[i];
    ++i;
}
```
4. Loop blocking: Loop blocking is not used for vectorization directly but rather serves as a performance booster by minimizing cache misses. Examples may include breaking the loops into blocks. For example.

#### Before:
```C++
void add(int a[][MAX], int b[][MAX]) {
    int i, j;
    for (i = 0; i < MAX; i++) {
        for (j = 0; j < MAX; j++ {
            a[i][j] = a[i][j] + b[j][i]; //Adds two matrices
        }
    }
}
```
#### After:
```C++
void add(int a[][MAX], int b[][MAX]) {
    int i, j, ii, jj;
    for (i = 0; i < MAX; i += BS) {
        for (j = 0; j < MAX; j += BS) {
            for (ii = i; ii < i + BS; ii++) { //outer loop
                for (jj = j; jj < j + BS; jj++) { 
                    //Array B experiences one cache miss
                    //for every iteration of outer loop
                    a[ii][jj] = a[ii][jj] + b[jj][ii]; //Add the two arrays
                }
            }
        }
    }
}
```
5. Loop Interchange and Subscripts: Sometimes loop interchange also helps with improving memory access patterns such as in the case of matrix multiplication.
<<<<<<< HEAD

## Part 4

For the optimisation we mainly followed the tutorials linked in the Literature.
The performance of the original code with optimizations enabled was at around 2 GFLOPS.
First we unrolled the outer loop in the dgemm_opt function to compute a 1x4 block.
Then we used registers and pointers for the computations. This improved the performance to ~4 GFLOPS.
Instead of computing a 1x4 block, we changed it to 4x4 blocks.
The outer loops now looked like this:
```C++
for ( j = 0; j < n; j += 4 ) { /* Loop over the columns of C, unrolled by 4 */
  for ( i = 0; i < m; i += 4 ) { /* Loop over the rows of C */
    /* Update C( i,j ), C( i,j+1 ), C( i,j+2 ), and C( i,j+3 ) in
    one routine (four inner products) */

    innerKernel_4x4( k, &A( i, 0 ), lda, &B( 0, j ), ldb, &C( i, j ), ldc );
  }
}
```

Then we switched from using standard registers to vector registers using intrinsics.
Using 128 bit vectors improved the performance to ~5 GFLOPS.
Using 256 bit vectors improved it to ~7 GFLOPS.
We then switched to 512 bit registers as KNL has 2 VPUs per core which means it can execute 2 512bit instructions per clock.
In order to do that we switched from computing 4x4 blocks to computing 4x8 blocks.
```C++
for ( j = 0; j < n; j += 8 ) { /* Loop over the columns of C, unrolled by 8 */
  for ( i = 0; i < m; i += 4 ) { /* Loop over the rows of C */
    innerKernel_4x8( k, &A( i, 0 ), lda, &B( 0, j ), ldb, &C( i, j ), ldc );
  }
}
```

This improved the performance to about 13 GFLOPS.
Because the switch from 1x4 to 4x4 blocks improved the performance, we tried to compute 8x8 blocks instead of 4x8 blocks.
We also started using intrinsics for adding __mm512_add_pd_ and multiplication __mm512_mul_pd_.
This improved the performance to ~20 GFLOPS, after we also changed the Alignment to 64.

We tried to improve the performance more by unrolling the inner loop in the _inner512_8x8_ kernel.
The largest improvement was achieved when unrulling the loop by 4.
The performance then was ~23 Gflops.
```C++
void inner512_8x8(int K_, double* A, int lda, double* B, int ldb, double* C, int ldc) {
  /** Declare 512d vector registers **/
  for (int k = 0; k < K_; k+=4) { // Step by 4 
      /** Same computation as before now unrolled by 4 **/
  }
}
```

To keep performance up for larger problem sizes, we added code to divide the matrices into smaller blocks.

Literature suggests that packing the matrices A and B into contiguous memory can improve the performance quite a lot, so we tried to do that.
Despite trying different things, we weren't able to achieve a performance gain doing this.
Performance even decreased.
Our code for packing is still in the file, but it is not used.

In the end the peak performance that we achieved was around 23 GFLOPS.

An output of a run can be found in _myjob.7259.mpp3r01c04s10.out_.

